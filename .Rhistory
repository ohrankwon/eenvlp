list.fit[[4]] = stenv(X.train.pca, Y.train, q=chosen.qu[1], u=chosen.qu[2])
betahat <- t(list.fit[[4]]$beta)
muhat <- list.fit[[4]]$mu
resi <- as.matrix(Y.test - matrix(1, 1, 1) %*% t(muhat) - t(as.matrix(X.test.pca)) %*% t(betahat))
sprederr <- apply(resi, 1, function(x) sum(x ^ 2))
list.prederr[[4]] <- c(list.prederr[[ind.method]], (sprederr/r) )
# PLSR
pls.mod = plsr(Y.train~X.train,validation="CV",segments=dim(X)[1]-2)
pls.rmsep = RMSEP(pls.mod)$val[1,,] # if [1,,], CV using root mean squared error of prediction (RMSE)
pls.num.comp = which.min(colSums(pls.rmsep^2)) - 1
pls.pred = predict(pls.mod,ncomp=pls.num.comp,newdata=matrix(X.test,1,length(X.test)),type="response")
list.prederr[[5]] <- c(list.prederr[[5]], sum((pls.pred[,,1]-Y.test)^2)/r )
}
## Prediction errors
print( round(unlist(lapply(list.prederr, mean)),3) )
library(eenvlp)
library(pls)
library(Renvlp)
data(cereal)
X = cereal$x
Y = cereal$y
r = dim(Y)[2]
n = dim(Y)[1]
list.prederr = list(fit.eenv = c(),
fit.env = c(),
fit.ridge = c(),
fit.adhocenv = c(),
fit.pls = c())
list.fit = list(fit.eenv = c(),
fit.env = c(),
fit.ridge = c(),
fit.adhocenv = c(),
fit.pls = c())
for(rep.num in 1:n){
set.seed(rep.num*12+789)
# Training/Testing Sets
X.train <- X[-rep.num, ]
Y.train <- Y[-rep.num, ]
X.test <- X[rep.num, ]
Y.test <- Y[rep.num, ]
X.train = scale(X.train, center=TRUE, scale=TRUE)
X.test = scale(matrix(X.test,1,dim(X)[2]), center=attributes(X.train)$`scaled:center`, scale=attributes(X.train)$`scaled:scale`)
#----- For eenv, env, and ridge
lambda.seq = 10^(seq(3, -3, length.out=20))
u.seq = c(0:r)
ind.cv = sample(n-1, n-1)
# Choosing u for envelope
cv.u.env = c()
for(u.dim in u.seq){
cv.u.env = c(cv.u.env, eenvlp::cv_eenvlp(X.train, Y.train, u=u.dim, lamb=1e-8, m=n-2, nperm=1, index=ind.cv)) #loocv
}
chosen.u.env = u.seq[which.min(cv.u.env)[1]]
# Choosing u and lamb for enhanced envelope
cv.lamb.env = matrix(NA,length(lambda.seq),length(u.seq))
for(i in 1:length(lambda.seq)){
for(j in 1:length(u.seq)){
cv.lamb.env[i,j] = eenvlp::cv_eenvlp(X.train, Y.train, u=u.seq[j], lamb=lambda.seq[i], m=n-2, nperm=1, index=ind.cv) #loocv
}
}
chosen = which(cv.lamb.env == min(cv.lamb.env), arr.ind=TRUE)
chosen.lamb.eenv = lambda.seq[chosen[1]]
chosen.u.eenv = u.seq[chosen[2]]
print(chosen.lamb.eenv)
print(lambda.seq[which.min(cv.lamb.env[,length(u.seq)])])
# Estimators
list.fit[[1]] = eenvlp::eenvlp(X.train, Y.train, u=chosen.u.eenv, lamb=chosen.lamb.eenv) # enhanced envelope
list.fit[[2]] = eenvlp::eenvlp(X.train, Y.train, u=chosen.u.env, lamb=1e-08) # envelope
list.fit[[3]] = eenvlp::eenvlp(X.train, Y.train, u=r, lamb=lambda.seq[which.min(cv.lamb.env[,length(u.seq)])]) # ridge
# Prediction
for(ind.method in c(1:3)){
Y.pred <- eenvlp::pred_eenvlp(list.fit[[ind.method]], X.test)
resi <- as.matrix(Y.test - Y.pred)
#sprederr <- apply(resi, 1, function(x) sum(x ^ 2))
list.prederr[[ind.method]] <- c(list.prederr[[ind.method]], (sum(resi^2)/r) ) ## mean squared error
}
#----- For PLSR and Adhoc env
# Adhoc envelope
pca <- prcomp(data.frame(X)[-rep.num,])
n.pca = which(summary(pca)[[6]][3,]>=0.995)[1]
X.train.pca <- predict(pca, newdata=data.frame(X)[-rep.num,])[,1:n.pca]
X.test.pca <- predict(pca, newdata=data.frame(X)[rep.num,])[,1:n.pca]
cv.u.env = matrix(NA,n.pca+1,r+1)
for(u.dim in c(0:r)){
for(q.dim in c(0:n.pca)){
cv.u.env[(q.dim+1),(u.dim+1)] = cv.stenv(X.train.pca, Y.train, q=q.dim, u=u.dim, m=dim(X)[1]-2, nperm=1)
}
}
chosen.qu = which(cv.u.env==min(cv.u.env), arr.ind=T)-1
list.fit[[4]] = stenv(X.train.pca, Y.train, q=chosen.qu[1], u=chosen.qu[2])
betahat <- t(list.fit[[4]]$beta)
muhat <- list.fit[[4]]$mu
resi <- as.matrix(Y.test - matrix(1, 1, 1) %*% t(muhat) - t(as.matrix(X.test.pca)) %*% t(betahat))
sprederr <- apply(resi, 1, function(x) sum(x ^ 2))
list.prederr[[4]] <- c(list.prederr[[ind.method]], (sprederr/r) )
# PLSR
pls.mod = plsr(Y.train~X.train,validation="CV",segments=dim(X)[1]-2)
pls.rmsep = RMSEP(pls.mod)$val[1,,] # if [1,,], CV using root mean squared error of prediction (RMSE)
pls.num.comp = which.min(colSums(pls.rmsep^2)) - 1
pls.pred = predict(pls.mod,ncomp=pls.num.comp,newdata=matrix(X.test,1,length(X.test)),type="response")
list.prederr[[5]] <- c(list.prederr[[5]], sum((pls.pred[,,1]-Y.test)^2)/r )
}
## Prediction errors
print( round(unlist(lapply(list.prederr, mean)),3) )
data(cereal)
head(cereal)
dim(cereal)
library(eenvlp)
library(pls)
library(Renvlp)
data(cereal)
X = cereal$x
Y = cereal$y
r = dim(Y)[2]
n = dim(Y)[1]
list.prederr = list(fit.eenv = c(),
fit.env = c(),
fit.ridge = c(),
fit.adhocenv = c(),
fit.pls = c())
list.fit = list(fit.eenv = c(),
fit.env = c(),
fit.ridge = c(),
fit.adhocenv = c(),
fit.pls = c())
for(rep.num in 1:n){
set.seed(rep.num*12+3456789)
# Training/Testing Sets
X.train <- X[-rep.num, ]
Y.train <- Y[-rep.num, ]
X.test <- X[rep.num, ]
Y.test <- Y[rep.num, ]
#X.train = scale(X.train, center=TRUE, scale=TRUE)
#X.test = scale(matrix(X.test,1,dim(X)[2]), center=attributes(X.train)$`scaled:center`, scale=attributes(X.train)$`scaled:scale`)
#----- For eenv, env, and ridge
lambda.seq = 10^(seq(-1, -7, length.out=20))
u.seq = c(0:r)
ind.cv = sample(n-1, n-1)
# Choosing u for envelope
cv.u.env = c()
for(u.dim in u.seq){
cv.u.env = c(cv.u.env, eenvlp::cv_eenvlp(X.train, Y.train, u=u.dim, lamb=1e-8, m=n-2, nperm=1, index=ind.cv)) #loocv
}
chosen.u.env = u.seq[which.min(cv.u.env)[1]]
# Choosing u and lamb for enhanced envelope
cv.lamb.env = matrix(NA,length(lambda.seq),length(u.seq))
for(i in 1:length(lambda.seq)){
for(j in 1:length(u.seq)){
cv.lamb.env[i,j] = eenvlp::cv_eenvlp(X.train, Y.train, u=u.seq[j], lamb=lambda.seq[i], m=n-2, nperm=1, index=ind.cv) #loocv
}
}
chosen = which(cv.lamb.env == min(cv.lamb.env), arr.ind=TRUE)
chosen.lamb.eenv = lambda.seq[chosen[1]]
chosen.u.eenv = u.seq[chosen[2]]
# Estimators
list.fit[[1]] = eenvlp::eenvlp(X.train, Y.train, u=chosen.u.eenv, lamb=chosen.lamb.eenv) # enhanced envelope
list.fit[[2]] = eenvlp::eenvlp(X.train, Y.train, u=chosen.u.env, lamb=1e-08) # envelope
list.fit[[3]] = eenvlp::eenvlp(X.train, Y.train, u=r, lamb=lambda.seq[which.min(cv.lamb.env[,length(u.seq)])]) # ridge
# Prediction
for(ind.method in c(1:3)){
Y.pred <- eenvlp::pred_eenvlp(list.fit[[ind.method]], X.test)
resi <- as.matrix(Y.test - Y.pred)
#sprederr <- apply(resi, 1, function(x) sum(x ^ 2))
list.prederr[[ind.method]] <- c(list.prederr[[ind.method]], (sum(resi^2)/r) ) ## mean squared error
}
#----- For PLSR and Adhoc env
# Adhoc envelope
pca <- prcomp(data.frame(X)[-rep.num,])
n.pca = which(summary(pca)[[6]][3,]>=0.995)[1]
X.train.pca <- predict(pca, newdata=data.frame(X)[-rep.num,])[,1:n.pca]
X.test.pca <- predict(pca, newdata=data.frame(X)[rep.num,])[,1:n.pca]
cv.u.env = matrix(NA,n.pca+1,r+1)
for(u.dim in c(0:r)){
for(q.dim in c(0:n.pca)){
cv.u.env[(q.dim+1),(u.dim+1)] = cv.stenv(X.train.pca, Y.train, q=q.dim, u=u.dim, m=dim(X)[1]-2, nperm=1)
}
}
chosen.qu = which(cv.u.env==min(cv.u.env), arr.ind=T)-1
list.fit[[4]] = stenv(X.train.pca, Y.train, q=chosen.qu[1], u=chosen.qu[2])
betahat <- t(list.fit[[4]]$beta)
muhat <- list.fit[[4]]$mu
resi <- as.matrix(Y.test - matrix(1, 1, 1) %*% t(muhat) - t(as.matrix(X.test.pca)) %*% t(betahat))
sprederr <- apply(resi, 1, function(x) sum(x ^ 2))
list.prederr[[4]] <- c(list.prederr[[ind.method]], (sprederr/r) )
# PLSR
pls.mod = plsr(Y.train~X.train,validation="CV",segments=dim(X)[1]-2)
pls.rmsep = RMSEP(pls.mod)$val[1,,] # if [1,,], CV using root mean squared error of prediction (RMSE)
pls.num.comp = which.min(colSums(pls.rmsep^2)) - 1
pls.pred = predict(pls.mod,ncomp=pls.num.comp,newdata=matrix(X.test,1,length(X.test)),type="response")
list.prederr[[5]] <- c(list.prederr[[5]], sum((pls.pred[,,1]-Y.test)^2)/r )
}
## Prediction errors
print( round(unlist(lapply(list.prederr, mean)),3) )
data(cereal) # load data
head(cereal$y)
data(cereal) # load data
head(cereal$y)
dim(cereal)
data(cereal) # load data
head(cereal$y)
dim(cereal$x)
data(cereal) # load data
head(cereal$y,2)
dim(cereal$x)
data(cereal) # load data
dim(cereal$x)
head(cereal$y,2)
sample(1:15,10)
library(eenvlp)
library(pls)
library(Renvlp)
data(cereal)
X = cereal$x
Y = cereal$y
r = dim(Y)[2]
n = dim(Y)[1]
list.prederr = list(fit.eenv = c(),
fit.env = c(),
fit.ridge = c(),
fit.adhocenv = c(),
fit.pls = c())
list.fit = list(fit.eenv = c(),
fit.env = c(),
fit.ridge = c(),
fit.adhocenv = c(),
fit.pls = c())
for(rep.num in 1:n){
set.seed(rep.num*12+3456789)
# Training/Testing Sets
X.train <- X[-rep.num, ]
Y.train <- Y[-rep.num, ]
X.test <- X[rep.num, ]
Y.test <- Y[rep.num, ]
aa = sample(1:15,10)
X.train <- X[aa, ]
Y.train <- Y[aa, ]
X.test <- X[-aa, ]
Y.test <- Y[-aa, ]
X.train = scale(X.train, center=TRUE, scale=TRUE)
X.test = scale(matrix(X.test,1,dim(X)[2]), center=attributes(X.train)$`scaled:center`, scale=attributes(X.train)$`scaled:scale`)
#----- For eenv, env, and ridge
lambda.seq = 10^(seq(-1, -7, length.out=20))
u.seq = c(0:r)
ind.cv = sample(n-1, n-1)
# Choosing u for envelope
cv.u.env = c()
for(u.dim in u.seq){
cv.u.env = c(cv.u.env, eenvlp::cv_eenvlp(X.train, Y.train, u=u.dim, lamb=1e-8, m=n-2, nperm=1, index=ind.cv)) #loocv
}
chosen.u.env = u.seq[which.min(cv.u.env)[1]]
# Choosing u and lamb for enhanced envelope
cv.lamb.env = matrix(NA,length(lambda.seq),length(u.seq))
for(i in 1:length(lambda.seq)){
for(j in 1:length(u.seq)){
cv.lamb.env[i,j] = eenvlp::cv_eenvlp(X.train, Y.train, u=u.seq[j], lamb=lambda.seq[i], m=n-2, nperm=1, index=ind.cv) #loocv
}
}
chosen = which(cv.lamb.env == min(cv.lamb.env), arr.ind=TRUE)
chosen.lamb.eenv = lambda.seq[chosen[1]]
chosen.u.eenv = u.seq[chosen[2]]
# Estimators
list.fit[[1]] = eenvlp::eenvlp(X.train, Y.train, u=chosen.u.eenv, lamb=chosen.lamb.eenv) # enhanced envelope
list.fit[[2]] = eenvlp::eenvlp(X.train, Y.train, u=chosen.u.env, lamb=1e-08) # envelope
list.fit[[3]] = eenvlp::eenvlp(X.train, Y.train, u=r, lamb=lambda.seq[which.min(cv.lamb.env[,length(u.seq)])]) # ridge
# Prediction
for(ind.method in c(1:3)){
Y.pred <- eenvlp::pred_eenvlp(list.fit[[ind.method]], X.test)
resi <- as.matrix(Y.test - Y.pred)
#sprederr <- apply(resi, 1, function(x) sum(x ^ 2))
list.prederr[[ind.method]] <- c(list.prederr[[ind.method]], (sum(resi^2)/r) ) ## mean squared error
}
#----- For PLSR and Adhoc env
# Adhoc envelope
pca <- prcomp(data.frame(X)[-rep.num,])
n.pca = which(summary(pca)[[6]][3,]>=0.995)[1]
X.train.pca <- predict(pca, newdata=data.frame(X)[-rep.num,])[,1:n.pca]
X.test.pca <- predict(pca, newdata=data.frame(X)[rep.num,])[,1:n.pca]
cv.u.env = matrix(NA,n.pca+1,r+1)
for(u.dim in c(0:r)){
for(q.dim in c(0:n.pca)){
cv.u.env[(q.dim+1),(u.dim+1)] = cv.stenv(X.train.pca, Y.train, q=q.dim, u=u.dim, m=dim(X)[1]-2, nperm=1)
}
}
chosen.qu = which(cv.u.env==min(cv.u.env), arr.ind=T)-1
list.fit[[4]] = stenv(X.train.pca, Y.train, q=chosen.qu[1], u=chosen.qu[2])
betahat <- t(list.fit[[4]]$beta)
muhat <- list.fit[[4]]$mu
resi <- as.matrix(Y.test - matrix(1, 1, 1) %*% t(muhat) - t(as.matrix(X.test.pca)) %*% t(betahat))
sprederr <- apply(resi, 1, function(x) sum(x ^ 2))
list.prederr[[4]] <- c(list.prederr[[ind.method]], (sprederr/r) )
# PLSR
pls.mod = plsr(Y.train~X.train,validation="CV",segments=dim(X)[1]-2)
pls.rmsep = RMSEP(pls.mod)$val[1,,] # if [1,,], CV using root mean squared error of prediction (RMSE)
pls.num.comp = which.min(colSums(pls.rmsep^2)) - 1
pls.pred = predict(pls.mod,ncomp=pls.num.comp,newdata=matrix(X.test,1,length(X.test)),type="response")
list.prederr[[5]] <- c(list.prederr[[5]], sum((pls.pred[,,1]-Y.test)^2)/r )
}
## Prediction errors
print( round(unlist(lapply(list.prederr, mean)),3) )
X_train <- X[1:14,] # the first 25 weeks of the year
Y_train <- Y[1:14,]
X_test <- X[15,] # the remaining 26 weeks of the year
Y_test <- Y[15,]
numcol(X_test)
data(cereal) # load data
n <- nrow(cereal$x)[1]
r <- ncol(ceral$y)[2]
data(cereal) # load data
n <- nrow(cereal$x)[1]
r <- ncol(cereal$y)[2]
p <- ncol(cereal$x)[2]
X_train <- X[1:14,] # the first 25 weeks of the year
Y_train <- Y[1:14,]
X_test <- X[15,] # the remaining 26 weeks of the year
Y_test <- Y[15,]
lambda.seq = 10^(seq(3, -3, length.out=20)) ## already done below
u.seq = c(1:r)
data(cereal) # load data
n <- nrow(cereal$x)[1]
r <- ncol(cereal$y)[2]
p <- ncol(cereal$x)[2]
lambda.seq = 10^(seq(3, -3, length.out=20)) ## already done below
u.seq = c(1:r)
r
ncol(cereal$y)[2]
dim(cereal$y)
data(cereal) # load data
n <- nrow(cereal$x)
r <- ncol(cereal$y)
p <- ncol(cereal$x)
r
lambda.seq = 10^(seq(3, -3, length.out=20)) ## already done below
u.seq = c(1:r)
cv.lamb.env = matrix(NA,length(lambda.seq),length(u.seq))
n
set.seed(1111)
for(i in 1:length(lambda.seq)){
for(j in 1:length(u.seq)){
cv.lamb.env[i,j] <- cv_eenvlp(X_train, Y_train, u=u.seq[j], lamb=lambda.seq[i], m=n-1, nperm=1)
}
}
X_train <- X[1:14,] # the first 25 weeks of the year
Y_train <- Y[1:14,]
X_test <- X[15,] # the remaining 26 weeks of the year
Y_test <- Y[15,]
lambda.seq = 10^(seq(3, -3, length.out=20)) ## already done below
u.seq = c(1:r)
cv.lamb.env = matrix(NA,length(lambda.seq),length(u.seq))
set.seed(1111)
for(i in 1:length(lambda.seq)){
for(j in 1:length(u.seq)){
cv.lamb.env[i,j] <- cv_eenvlp(X_train, Y_train, u=u.seq[j], lamb=lambda.seq[i], m=n-1, nperm=1)
}
}
set.seed(1111)
for(i in 1:length(lambda.seq)){
for(j in 1:length(u.seq)){
cv.lamb.env[i,j] <- cv_eenvlp(X_train, Y_train, u=u.seq[j], lamb=lambda.seq[i], m=n-2, nperm=1)
}
}
chosen <- which(cv.lamb.env == min(cv.lamb.env), arr.ind=TRUE)
chosen.lamb <- lambda.seq[chosen[1]]
chosen.u <- u.seq[chosen[2]]
print(c(chosen.lamb,chosen.u))
cv_eenvlp
set.seed(1111)
for(i in 1:length(lambda.seq)){
for(j in 1:length(u.seq)){
cv.lamb.env[i,j] <- cv_eenvlp(X_train, Y_train, u=u.seq[j], lamb=lambda.seq[i], m=10, nperm=1)
}
}
chosen <- which(cv.lamb.env == min(cv.lamb.env), arr.ind=TRUE)
chosen.lamb <- lambda.seq[chosen[1]]
chosen.u <- u.seq[chosen[2]]
print(c(chosen.lamb,chosen.u))
set.seed(1111)
for(i in 1:length(lambda.seq)){
for(j in 1:length(u.seq)){
cv.lamb.env[i,j] <- cv_eenvlp(X_train, Y_train, u=u.seq[j], lamb=lambda.seq[i], m=10, nperm=1)
}
}
chosen <- which(cv.lamb.env == min(cv.lamb.env), arr.ind=TRUE)
chosen.lamb <- lambda.seq[chosen[1]]
chosen.u <- u.seq[chosen[2]]
print(c(chosen.lamb,chosen.u))
set.seed(11111)
for(i in 1:length(lambda.seq)){
for(j in 1:length(u.seq)){
cv.lamb.env[i,j] <- cv_eenvlp(X_train, Y_train, u=u.seq[j], lamb=lambda.seq[i], m=10, nperm=1)
}
}
chosen <- which(cv.lamb.env == min(cv.lamb.env), arr.ind=TRUE)
chosen.lamb <- lambda.seq[chosen[1]]
chosen.u <- u.seq[chosen[2]]
print(c(chosen.lamb,chosen.u))
n
data(cereal) # load data
n <- nrow(cereal$x)
r <- ncol(cereal$y)
p <- ncol(cereal$x)
X_train <- X[-1,] # the first 25 weeks of the year
Y_train <- Y[-1,]
X_test <- X[1,] # the remaining 26 weeks of the year
Y_test <- Y[1,]
lambda.seq = 10^(seq(3, -3, length.out=20)) ## already done below
u.seq = c(1:r)
cv.lamb.env = matrix(NA,length(lambda.seq),length(u.seq))
set.seed(11111)
for(i in 1:length(lambda.seq)){
for(j in 1:length(u.seq)){
cv.lamb.env[i,j] <- cv_eenvlp(X_train, Y_train, u=u.seq[j], lamb=lambda.seq[i], m=10, nperm=1)
}
}
chosen <- which(cv.lamb.env == min(cv.lamb.env), arr.ind=TRUE)
chosen.lamb <- lambda.seq[chosen[1]]
chosen.u <- u.seq[chosen[2]]
print(c(chosen.lamb,chosen.u))
set.seed(11111)
for(i in 1:length(lambda.seq)){
for(j in 1:length(u.seq)){
cv.lamb.env[i,j] <- cv_eenvlp(X_train, Y_train, u=u.seq[j], lamb=lambda.seq[i], m=10, nperm=1)
}
}
chosen <- which(cv.lamb.env == min(cv.lamb.env), arr.ind=TRUE)
chosen.lamb <- lambda.seq[chosen[1]]
chosen.u <- u.seq[chosen[2]]
print(c(chosen.lamb,chosen.u))
fit_eenvlp <- eenvlp(X_train, Y_train, u = chosen.u, lamb = chosen.lamb) #chosen.lamb)
fit_eenvlp2 <- eenvlp(X_train, Y_train, u = 2, lamb = 1e-08)
predict_eenvlp <- pred_eenvlp(fit_eenvlp, X_test)
predict_eenvlp2 <- pred_eenvlp(fit_eenvlp2, X_test)
(predict_eenvlp - Y_test)^2
(predict_eenvlp2 - Y_test)^2
fit_eenvlp <- eenvlp(X_train, Y_train, u = chosen.u, lamb = chosen.lamb) #chosen.lamb)
fit_eenvlp2 <- eenvlp(X_train, Y_train, u = 1, lamb = 1e-08)
predict_eenvlp <- pred_eenvlp(fit_eenvlp, X_test)
predict_eenvlp2 <- pred_eenvlp(fit_eenvlp2, X_test)
(predict_eenvlp - Y_test)^2
(predict_eenvlp2 - Y_test)^2
fit_eenvlp <- eenvlp(X_train, Y_train, u = chosen.u, lamb = chosen.lamb) #chosen.lamb)
fit_eenvlp2 <- eenvlp(X_train, Y_train, u = 1, lamb = 1e-10)
predict_eenvlp <- pred_eenvlp(fit_eenvlp, X_test)
predict_eenvlp2 <- pred_eenvlp(fit_eenvlp2, X_test)
(predict_eenvlp - Y_test)^2
(predict_eenvlp2 - Y_test)^2
(predict_eenvlp - Y_test)^2
data(cereal) # load data
n <- nrow(cereal$x)
r <- ncol(cereal$y)
p <- ncol(cereal$x)
X_test <- cereal$x[1,] # the first observation
Y_test <- cereal$y[1,]
X_train <- cereal$x[-1,] # the remaining observations
Y_train <- cereal$xy[-1,]
lambda.seq = 10^(seq(3, -3, length.out=20))
u.seq = c(1:r)
cv.lamb.env = matrix(NA,length(lambda.seq),length(u.seq))
set.seed(11111)
for(i in 1:length(lambda.seq)){
for(j in 1:length(u.seq)){
cv.lamb.env[i,j] <- cv_eenvlp(X_train, Y_train, u=u.seq[j], lamb=lambda.seq[i], m=10, nperm=1)
}
}
library(devtools)
devtools::install_github("ohrankwon/eenvlp")
library(eenvlp)
data(cereal) # load data
n <- nrow(cereal$x) # Number of observations
r <- ncol(cereal$y) # Number of response variables
p <- ncol(cereal$x) # Number of predictors
print(c(n,r,p))
# Define training and test sets
X_test <- cereal$x[1,] # The first observation as the test set
Y_test <- cereal$y[1,]
X_train <- cereal$x[-1,] # The remaining observations as the training set
Y_train <- cereal$y[-1,]
lambda.seq = 10^(seq(3, -3, length.out=20)) # Sequence of lambda values
u.seq = c(1:r) # Sequence of u values
cv.lamb.env = matrix(NA,length(lambda.seq),length(u.seq))
set.seed(11111) # Set a seed for reproducibility
for(i in 1:length(lambda.seq)){
for(j in 1:length(u.seq)){
cv.lamb.env[i,j] <- cv_eenvlp(X_train, Y_train, u=u.seq[j], lamb=lambda.seq[i], m=10, nperm=1)
}
}
# Identify the optimal parameters
chosen <- which(cv.lamb.env == min(cv.lamb.env), arr.ind=TRUE)
chosen.lamb <- lambda.seq[chosen[1]]
chosen.u <- u.seq[chosen[2]]
cat("Optimal lambda:", chosen.lamb, "\n")
cat("Optimal u:", chosen.u, "\n")
fit_eenvlp <- eenvlp(X_train, Y_train, u = chosen.u, lamb = chosen.lamb)
predict_eenvlp <- pred_eenvlp(fit_eenvlp, X_test)
predict_eenvlp <- pred_eenvlp(fit_eenvlp, X_test)
prediction_error <- (predict_eenvlp - Y_test)^2
prediction_error
pkgdown::build_site()
pkgdown::build_site()
pkgdown::build_site()
pkgdown::build_site()
pkgdown::build_site()
options(rmarkdown.html_vignette.check_title = FALSE)
pkgdown::build_site()
pkgdown::build_site()
pkgdown::build_site()
pkgdown::build_site()
View(predict_eenvlp3)
pkgdown::build_site()
pkgdown::build_site()
pkgdown::build_site()
pkgdown::build_site()
pkgdown::build_site()
pkgdown::build_site()
