
R version 4.3.1 (2023-06-16) -- "Beagle Scouts"
Copyright (C) 2023 The R Foundation for Statistical Computing
Platform: aarch64-apple-darwin20 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> library(eenvlp)
> 
> library(pls)
> library(Renvlp)
> 
> data(cereal)
> 
> X = cereal$x
> Y = cereal$y
> 
> r = dim(Y)[2]
> n = dim(Y)[1]
> 
> list.prederr = list(fit.eenv = c(),
+                     fit.env = c(),
+                     fit.ridge = c(),
+                     fit.adhocenv = c(),
+                     fit.pls = c())
> list.fit = list(fit.eenv = c(),
+                     fit.env = c(),
+                     fit.ridge = c(),
+                     fit.adhocenv = c(),
+                     fit.pls = c())
> 
> for(rep.num in 1:n){
+   
+   set.seed(rep.num*12+3456789)
+   
+   # Training/Testing Sets
+ 
+   X.train <- X[-rep.num, ]
+   Y.train <- Y[-rep.num, ]
+   X.test <- X[rep.num, ]
+   Y.test <- Y[rep.num, ]
+ 
+   X.train = scale(X.train, center=TRUE, scale=TRUE)
+   X.test = scale(matrix(X.test,1,dim(X)[2]), center=attributes(X.train)$`scaled:center`, scale=attributes(X.train)$`scaled:scale`)
+   
+   #----- For eenv, env, and ridge
+   
+   lambda.seq = 10^(seq(-1, -7, length.out=20))
+   u.seq = c(0:r)
+   ind.cv = sample(n-1, n-1)
+ 
+   # Choosing u for envelope
+   cv.u.env = c()
+   for(u.dim in u.seq){
+     cv.u.env = c(cv.u.env, eenvlp::cv_eenvlp(X.train, Y.train, u=u.dim, lamb=1e-8, m=n-2, nperm=1, index=ind.cv)) #loocv
+   }
+   chosen.u.env = u.seq[which.min(cv.u.env)[1]]
+ 
+   # Choosing u and lamb for enhanced envelope
+   cv.lamb.env = matrix(NA,length(lambda.seq),length(u.seq))
+   for(i in 1:length(lambda.seq)){
+     for(j in 1:length(u.seq)){
+       cv.lamb.env[i,j] = eenvlp::cv_eenvlp(X.train, Y.train, u=u.seq[j], lamb=lambda.seq[i], m=n-2, nperm=1, index=ind.cv) #loocv
+     }
+   }
+   chosen = which(cv.lamb.env == min(cv.lamb.env), arr.ind=TRUE)
+   chosen.lamb.eenv = lambda.seq[chosen[1]]
+   chosen.u.eenv = u.seq[chosen[2]]
+ 
+   # Estimators
+   list.fit[[1]] = eenvlp::eenvlp(X.train, Y.train, u=chosen.u.eenv, lamb=chosen.lamb.eenv) # enhanced envelope
+   list.fit[[2]] = eenvlp::eenvlp(X.train, Y.train, u=chosen.u.env, lamb=1e-08) # envelope
+   list.fit[[3]] = eenvlp::eenvlp(X.train, Y.train, u=r, lamb=lambda.seq[which.min(cv.lamb.env[,length(u.seq)])]) # ridge
+   
+   # Prediction
+   for(ind.method in c(1:3)){
+     Y.pred <- eenvlp::pred_eenvlp(list.fit[[ind.method]], X.test)
+     resi <- as.matrix(Y.test - Y.pred)
+     #sprederr <- apply(resi, 1, function(x) sum(x ^ 2))
+     list.prederr[[ind.method]] <- c(list.prederr[[ind.method]], (sum(resi^2)/r) ) ## mean squared error
+   }
+ 
+   #----- For PLSR and Adhoc env
+   
+   # Adhoc envelope
+   pca <- prcomp(data.frame(X)[-rep.num,])
+   n.pca = which(summary(pca)[[6]][3,]>=0.995)[1]
+   X.train.pca <- predict(pca, newdata=data.frame(X)[-rep.num,])[,1:n.pca]
+   X.test.pca <- predict(pca, newdata=data.frame(X)[rep.num,])[,1:n.pca]
+   
+   cv.u.env = matrix(NA,n.pca+1,r+1)
+   for(u.dim in c(0:r)){
+     for(q.dim in c(0:n.pca)){
+       cv.u.env[(q.dim+1),(u.dim+1)] = cv.stenv(X.train.pca, Y.train, q=q.dim, u=u.dim, m=dim(X)[1]-2, nperm=1)
+     }
+   }
+   
+   chosen.qu = which(cv.u.env==min(cv.u.env), arr.ind=T)-1
+   list.fit[[4]] = stenv(X.train.pca, Y.train, q=chosen.qu[1], u=chosen.qu[2])
+ 
+   betahat <- t(list.fit[[4]]$beta)
+   muhat <- list.fit[[4]]$mu
+   resi <- as.matrix(Y.test - matrix(1, 1, 1) %*% t(muhat) - t(as.matrix(X.test.pca)) %*% t(betahat))
+   sprederr <- apply(resi, 1, function(x) sum(x ^ 2))
+   list.prederr[[4]] <- c(list.prederr[[ind.method]], (sprederr/r) )
+   
+   # PLSR
+   pls.mod = plsr(Y.train~X.train,validation="CV",segments=dim(X)[1]-2)
+   pls.rmsep = RMSEP(pls.mod)$val[1,,] # if [1,,], CV using root mean squared error of prediction (RMSE)
+   pls.num.comp = which.min(colSums(pls.rmsep^2)) - 1
+   pls.pred = predict(pls.mod,ncomp=pls.num.comp,newdata=matrix(X.test,1,length(X.test)),type="response")
+   list.prederr[[5]] <- c(list.prederr[[5]], sum((pls.pred[,,1]-Y.test)^2)/r )
+ 
+ }
> 
> ## Prediction errors
> print( round(unlist(lapply(list.prederr, mean)),3) )
    fit.eenv      fit.env    fit.ridge fit.adhocenv      fit.pls 
       0.298        0.405        0.340        0.381        0.395 
> 
